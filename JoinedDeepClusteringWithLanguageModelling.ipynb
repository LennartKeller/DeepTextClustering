{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "torch.cuda.is_available(), torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some helping stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def cluster_accuracy(y_true, y_predicted, cluster_number: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy after using the linear_sum_assignment function in SciPy to\n",
    "    determine reassignments.\n",
    "    :param y_true: list of true cluster numbers, an integer array 0-indexed\n",
    "    :param y_predicted: list  of predicted cluster numbers, an integer array 0-indexed\n",
    "    :param cluster_number: number of clusters, if None then calculated from input\n",
    "    :return: reassignment dictionary, clustering accuracy\n",
    "    \"\"\"\n",
    "    if cluster_number is None:\n",
    "        cluster_number = (\n",
    "            max(y_predicted.max(), y_true.max()) + 1\n",
    "        )  # assume labels are 0-indexed\n",
    "    count_matrix = np.zeros((cluster_number, cluster_number), dtype=np.int64)\n",
    "    for i in range(y_predicted.size):\n",
    "        count_matrix[y_predicted[i], y_true[i]] += 1\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(count_matrix.max() - count_matrix)\n",
    "    reassignment = dict(zip(row_ind, col_ind))\n",
    "    accuracy = count_matrix[row_ind, col_ind].sum() / y_predicted.size\n",
    "    return reassignment, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.texts[index], labels[index]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "data = fetch_20newsgroups(\n",
    "    categories=['comp.graphics', 'talk.religion.misc', 'rec.autos', 'sci.med'],\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "texts, labels = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataset(texts, labels)\n",
    "data_loader = DataLoader(dataset=data, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ClusteringLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, initial_centroids, alpha=1.0):\n",
    "        super(ClusteringLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(initial_centroids)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes a batch of input embeddings of shape (batch_size, embedding_size).\n",
    "        And computes of soft clustering based in on the centroids.\n",
    "        \"\"\"    \n",
    "        norm_squared = torch.sum((inputs.unsqueeze(1) - self.weights) ** 2, 2)\n",
    "        numerator = 1.0 / (1.0 + (norm_squared / self.alpha))\n",
    "        power = float(self.alpha + 1) / 2\n",
    "        q = numerator ** power\n",
    "        return q\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertForClustering(nn.Module):\n",
    "    \n",
    "    def __init__(self, distilbert_model, initial_centroids):\n",
    "        super(DistilBertForClustering, self).__init__()\n",
    "        self.distilbert_model = distilbert_model\n",
    "        self.initial_centroids = initial_centroids\n",
    "        self.clustering_layer = ClusteringLayer(initial_centroids=initial_centroids)\n",
    "        \n",
    "    def target_probability_distribution(self, q):\n",
    "        weight = (q ** 2) / torch.sum(q, 0)\n",
    "        return (weight.t() / torch.sum(weight, 1)).t()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        distilbert_outputs = self.distilbert_model(**inputs)\n",
    "        cls_embeddings = distilbert_outputs.last_hidden_state[:,0,:]\n",
    "        \n",
    "        q = self.clustering_layer(cls_embeddings)\n",
    "        p = self.target_probability_distribution(q)\n",
    "        return q, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init distilbert model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "lm_model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "lm_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = lm_model.base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain initial clustering in order to do initial clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2149it [00:20, 104.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embeddings = []\n",
    "for index, text in tqdm(enumerate(texts)):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to('cuda')\n",
    "    outputs = base_model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:,0,:].flatten().cpu().detach().numpy()\n",
    "    embeddings.append(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(np.array(embeddings).T[:].flatten(), bins=512)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0712, -0.0491,  0.0697,  ..., -0.1380,  0.3239,  0.2242],\n",
       "         [-0.0155,  0.0224, -0.1704,  ..., -0.0005,  0.4066,  0.3976],\n",
       "         [-0.1415, -0.0496,  0.0142,  ..., -0.0641,  0.1895,  0.2586],\n",
       "         [-0.1813, -0.0892,  0.0248,  ..., -0.0418,  0.3154,  0.4834]],\n",
       "        device='cuda:0', dtype=torch.float64),\n",
       " torch.Size([4, 768]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=np.unique(labels).shape[0], n_init=20)\n",
    "\n",
    "kmeans_cluster = kmeans.fit_predict(embeddings)\n",
    "kmeans_centroids = torch.from_numpy(kmeans.cluster_centers_)\n",
    "\n",
    "kmeans_centroids = kmeans_centroids.to('cuda')\n",
    "kmeans_centroids, kmeans_centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm = GaussianMixture(n_components=np.unique(labels).shape[0], covariance_type='full')\n",
    "gm_cluster = gm.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evluation as baseline for the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6845044206607724\n",
      "0.49862176101142586\n",
      "0.4252118960261368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "print(cluster_accuracy(labels, kmeans_cluster)[1])\n",
    "print(normalized_mutual_info_score(labels, kmeans_cluster))\n",
    "print(adjusted_rand_score(labels, kmeans_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6877617496510005\n",
      "0.4995262154012147\n",
      "0.43233156041813925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "print(cluster_accuracy(labels, gm_cluster)[1])\n",
    "print(normalized_mutual_info_score(labels, gm_cluster))\n",
    "print(adjusted_rand_score(labels, gm_cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForClustering(\n",
       "  (distilbert_model): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (clustering_layer): torch.Size([4, 768])\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_model = DistilBertForClustering(distilbert_model=base_model, initial_centroids=kmeans_centroids)\n",
    "cluster_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "lm_optimizer = torch.optim.Adam(params=lm_model.parameters(), lr=0.00001)\n",
    "cluster_optimizer = torch.optim.SGD(params=cluster_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_loss_fn = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kld(pred, target):\n",
    "            return torch.mean(torch.sum(target*torch.log(target/(pred+1e-6)), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(pred, target):\n",
    "    return torch.mean(torch.sum(target * torch.log(target/pred), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, t, lm_model, cluster_model, data_loader, lm_optimizer, cluster_optimizer, clustering_loss_fn):\n",
    "    \n",
    "    lm_model.train()\n",
    "    cluster_model.train()\n",
    "    \n",
    "    # train part\n",
    "    pbar = tqdm(data_loader)\n",
    "    for batch_index, batch_data in enumerate(pbar):\n",
    "        \n",
    "        if batch_index % t == 0:\n",
    "            pbar.set_description(\"Recomputing ps\")\n",
    "            # compute p for each batch and store it\n",
    "            ps = {}\n",
    "            for index, xi in enumerate(list(data_loader)[batch_index: batch_index + t]):\n",
    "                texts, _ = xi\n",
    "                inputs = tokenizer(\n",
    "                    texts,\n",
    "                    return_tensors='pt',\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                )\n",
    "                inputs = inputs.to('cuda')\n",
    "                _, p = cluster_model(inputs)\n",
    "                p = p.detach()\n",
    "                ps[batch_index + index] = p\n",
    "\n",
    "        inputs = inputs.to('cuda')\n",
    "        # get inputs for the models\n",
    "        texts, _ = batch_data\n",
    "        inputs = tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        inputs = inputs.to('cuda')\n",
    "        # do lm task\n",
    "        outputs = lm_model(labels=inputs['input_ids'], **inputs)\n",
    "        lm_loss = outputs.loss\n",
    "        # do clustering task\n",
    "        q, _ = cluster_model(inputs)\n",
    "        #q = q.cpu().detach()\n",
    "        p = ps[batch_index]\n",
    "        clustering_loss = clustering_loss_fn(q, p)\n",
    "        \n",
    "        # do optimization step\n",
    "        combined_loss = (.001 * lm_loss) + (1. * clustering_loss)\n",
    "        \n",
    "        lm_optimizer.zero_grad()\n",
    "        cluster_optimizer.zero_grad()\n",
    "        \n",
    "        combined_loss.backward()\n",
    "        \n",
    "        lm_optimizer.step()\n",
    "        cluster_optimizer.step()\n",
    "        \n",
    "        pbar.set_description(f'''Epoch {epoch + 1} | Combined Loss {combined_loss.item()} | Clustering Loss {clustering_loss.item()} | LM Loss {lm_loss.item()}''')\n",
    "    \n",
    "    # validation part\n",
    "    with torch.no_grad():\n",
    "        cluster_model.eval()\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        embeddings = []\n",
    "        for batch_index, batch_data in enumerate(pbar):\n",
    "            texts, labels = batch_data\n",
    "            inputs = tokenizer(\n",
    "                texts,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            inputs = inputs.to('cuda')\n",
    "            q, p = cluster_model(inputs)\n",
    "            embeddings.append(\n",
    "                cluster_model.distilbert_model.base_model(**inputs).last_hidden_state[:,0,:].cpu().detach().numpy()\n",
    "            )\n",
    "            \n",
    "            predicted_label = q.argmax(dim=1).cpu().detach().numpy()\n",
    "            true_label = labels.cpu().detach().numpy()\n",
    "            \n",
    "            predicted_labels.extend(predicted_label)\n",
    "            true_labels.extend(true_label)\n",
    "            \n",
    "        true_labels = np.array(true_labels).flatten()\n",
    "        predicted_labels = np.array(predicted_labels).flatten()\n",
    "\n",
    "        print('#' * 60)\n",
    "        print(\"NN Measures\")\n",
    "        print(\"NMI:\", normalized_mutual_info_score(true_labels, predicted_labels))\n",
    "        print(\"Accuracy:\", cluster_accuracy(true_labels, predicted_labels))\n",
    "        print(\"ARI:\", adjusted_rand_score(true_labels, predicted_labels))\n",
    "        \n",
    "        print(\"Kmeans measures\")\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        kmeans = KMeans(n_clusters=np.unique(true_labels).shape[0], n_init=20)\n",
    "        kmeans_predicted_labels = kmeans.fit_predict(embeddings)\n",
    "        print(\"NMI:\", normalized_mutual_info_score(true_labels, kmeans_predicted_labels))\n",
    "        print(\"Accuracy:\", cluster_accuracy(true_labels, kmeans_predicted_labels))\n",
    "        print(\"ARI:\", adjusted_rand_score(true_labels, kmeans_predicted_labels))\n",
    "        \n",
    "        return embeddings, true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recomputing ps:  49%|████▉     | 265/538 [00:49<00:51,  5.33it/s]                                                                                               "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    embeddings, true, pred = train(\n",
    "        epoch=epoch,\n",
    "        t=5,\n",
    "        lm_model=lm_model,\n",
    "        cluster_model=cluster_model,\n",
    "        data_loader=data_loader,\n",
    "        lm_optimizer=lm_optimizer,\n",
    "        cluster_optimizer=cluster_optimizer,\n",
    "        clustering_loss_fn=kl_divergence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(embeddings.T[:].flatten(), bins=512)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(embeddings).shape\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pred, return_counts=True), np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute embeddings\n",
    "from tqdm import tqdm\n",
    "tuned_embeddings = []\n",
    "for index, text in tqdm(enumerate(texts)):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to('cuda')\n",
    "    outputs = base_model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:,0,:].flatten().cpu().detach().numpy()\n",
    "    tuned_embeddings.append(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model.distilbert_model.base_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP \n",
    "\n",
    "pca = UMAP(n_components=2)\n",
    "\n",
    "Xr = pca.fit_transform(tuned_embeddings)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=Xr[:,0], y=Xr[:,1], hue=[f'C{i}' for i in kmeans_cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=np.unique(labels).shape[0], n_init=20)\n",
    "\n",
    "kmeans_cluster = kmeans.fit_predict(tuned_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "print(cluster_accuracy(labels, kmeans_cluster)[1])\n",
    "print(normalized_mutual_info_score(labels, kmeans_cluster))\n",
    "print(adjusted_rand_score(labels, kmeans_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exceptionception('Stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backup\n",
    "if batch_index % t == 0:\n",
    "            pbar.set_description(\"Recomputing ps\")\n",
    "            # compute p for each batch and store it\n",
    "            ps = []\n",
    "            for index, b in enumerate(data_loader):\n",
    "                texts, _ = b\n",
    "                inputs = tokenizer(\n",
    "                    texts,\n",
    "                    return_tensors='pt',\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                )\n",
    "                inputs = inputs.to('cuda')\n",
    "                _, p = cluster_model(inputs)\n",
    "                p = p.cpu().detach()\n",
    "                ps.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code for my training and p update strategy\n",
    "data_loader = DataLoader(list(range(1000)))\n",
    "t = 100\n",
    "for batch_index, x in enumerate(data):\n",
    "    if batch_index % t == 0:\n",
    "        xs = {}\n",
    "        for index, xi in enumerate(list(data_loader)[batch_index: batch_index + t]):\n",
    "            xs[batch_index + index] = xi\n",
    "    print(x == xs[batch_index])\n",
    "    print(len(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "Decimal(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
