{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a test bench for working on the core model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def cluster_accuracy(y_true, y_predicted, cluster_number: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy after using the linear_sum_assignment function in SciPy to\n",
    "    determine reassignments.\n",
    "    :param y_true: list of true cluster numbers, an integer array 0-indexed\n",
    "    :param y_predicted: list  of predicted cluster numbers, an integer array 0-indexed\n",
    "    :param cluster_number: number of clusters, if None then calculated from input\n",
    "    :return: reassignment dictionary, clustering accuracy\n",
    "    \"\"\"\n",
    "    if cluster_number is None:\n",
    "        cluster_number = (\n",
    "            max(y_predicted.max(), y_true.max()) + 1\n",
    "        )  # assume labels are 0-indexed\n",
    "    count_matrix = np.zeros((cluster_number, cluster_number), dtype=np.int64)\n",
    "    for i in range(y_predicted.size):\n",
    "        count_matrix[y_predicted[i], y_true[i]] += 1\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(count_matrix.max() - count_matrix)\n",
    "    reassignment = dict(zip(row_ind, col_ind))\n",
    "    accuracy = count_matrix[row_ind, col_ind].sum() / y_predicted.size\n",
    "    return reassignment, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.texts[index], self.labels[index]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_distance(X, Y, p=1):\n",
    "    \"\"\"\n",
    "    Computes row wise minkowski distances between matrices X and Y\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.abs(X-Y)**p, dim=1)**(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import *\n",
    "\n",
    "def mask_tokens(inputs: torch.Tensor, tokenizer) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "    \"\"\"\n",
    "\n",
    "    if tokenizer.mask_token is None:\n",
    "        raise ValueError(\n",
    "            \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
    "        )\n",
    "\n",
    "    labels = inputs.clone()\n",
    "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "    probability_matrix = torch.full(labels.shape, 0.15)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    if tokenizer._pad_token is not None:\n",
    "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sparsity(X):\n",
    "    \"\"\"\n",
    "    Computes the mean of the row-wise sparsity of the input feature-matrix X\n",
    "    \"\"\"\n",
    "    return np.mean(np.count_nonzero(X, axis=1) / X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sparsity(np.array([\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "@dataclass\n",
    "class ClusterOutput(ModelOutput):\n",
    "    \n",
    "    loss: torch.FloatTensor = None\n",
    "    predicted_labels: torch.IntTensor = None\n",
    "    embeddings: torch.FloatTensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterLM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 initial_centroids: torch.tensor,\n",
    "                 lm_model,\n",
    "                 tokenizer,\n",
    "                 metric=lp_distance,\n",
    "                 do_language_modeling=True,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "        super(ClusterLM, self).__init__()\n",
    "\n",
    "        self.initial_centroids = initial_centroids\n",
    "\n",
    "        self.add_module('lm_model', lm_model)\n",
    "        self.register_parameter('centroids', nn.Parameter(initial_centroids.clone().float(), requires_grad=True))\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.metric = metric\n",
    "        self.do_language_modeling = do_language_modeling\n",
    "        self.device = device\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, texts, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Input: texts and labels (optional)\n",
    "        Returns: lm_language modelling output, own output dict (clustering_loss, predicted_labels)\n",
    "        \"\"\"\n",
    "        # Language Modeling Part:\n",
    "\n",
    "        lm_outputs = None\n",
    "\n",
    "        if self.do_language_modeling:\n",
    "            inputs = self.tokenizer(\n",
    "                texts,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                truncation=True)\n",
    "\n",
    "            input_ids = inputs['input_ids'].clone()\n",
    "            input_ids, true_ids = mask_tokens(input_ids, self.tokenizer)\n",
    "            inputs['input_ids'] = input_ids\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            true_ids = true_ids.to(self.device)\n",
    "            lm_outputs = self.lm_model(labels=true_ids, **inputs)\n",
    "\n",
    "        # Clustering Part:\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True)\n",
    "\n",
    "        inputs.to(self.device)\n",
    "\n",
    "        # 0. Obtain embeddings for each input\n",
    "        input_embeddings = self.lm_model.base_model(**inputs).last_hidden_state[:, 0, :].float()\n",
    "\n",
    "        # 1. Compute distances from each input embedding to each centroids\n",
    "        distances = torch.stack([self.metric(embedding.unsqueeze(0), self.centroids) for embedding in input_embeddings])\n",
    "        nearest_centroids = torch.argmin(distances.cpu().clone().detach(), dim=1)\n",
    "        distances = torch.transpose(distances, 0, 1)  # => shape (n_centroids, n_samples)\n",
    "\n",
    "        # 2. Compute the paramterized softmin for each centroid of each distance to each centroid per input sample\n",
    "        # Find min distances for each centroid\n",
    "        min_distances = torch.min(distances, dim=1).values\n",
    "        # Compute exponetials\n",
    "        exponentials = torch.exp(- alpha * (distances - min_distances.unsqueeze(1)))\n",
    "        # Compute softmin\n",
    "        softmin = exponentials / torch.sum(exponentials, dim=1).unsqueeze(1)\n",
    "\n",
    "        # 3. Weight the distance between each sample and each centroid\n",
    "        weighted_distances = distances * softmin\n",
    "\n",
    "        # 4. Sum over weighted_distances to obtain loss\n",
    "        clustering_loss = weighted_distances.sum(dim=1).mean()\n",
    "\n",
    "        # Create clustering output dictionary\n",
    "        cluster_outputs = ClusterOutput(\n",
    "            loss=clustering_loss,\n",
    "            predicted_labels=nearest_centroids.long(),\n",
    "            embeddings=input_embeddings.cpu().detach()\n",
    "        )\n",
    "\n",
    "        return lm_outputs, cluster_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test params \n",
    "N_EPOCHS = 5\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/ag_news_subset5.csv')\n",
    "\n",
    "texts = df['texts'].to_numpy()\n",
    "labels = df['labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#texts, _, labels, _ = train_test_split(texts, labels, test_size=0.99, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataset(texts, labels)\n",
    "data_loader = DataLoader(dataset=data, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "lm_model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "lm_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "embeddings = []\n",
    "for index, text in enumerate(tqdm(texts)):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    outputs = lm_model.base_model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:,0,:].flatten().cpu().detach().numpy()\n",
    "    \n",
    "    embeddings.append(cls_embedding)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster.k_means_ import _k_init\n",
    "from sklearn.utils.extmath import row_norms\n",
    "\n",
    "# Using KMeans++ initialization\n",
    "initial_centroids = _k_init(\n",
    "    embeddings,\n",
    "    n_clusters=np.unique(labels).shape[0],\n",
    "    x_squared_norms=row_norms(embeddings, squared=True),  #aka np.linalg.norm(embeddings, axis=1)**2\n",
    "    random_state=np.random.RandomState(42))\n",
    "initial_centroids = torch.from_numpy(initial_centroids).to('cpu')\n",
    "initial_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClusterLM(initial_centroids=initial_centroids, lm_model=lm_model, tokenizer=tokenizer, device=DEVICE)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=2e-5, #2e-5, 5e-7, 5e-10\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "emb_hist = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    batch_embs = []\n",
    "    \n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(data_loader)\n",
    "    for texts, labels in pbar:\n",
    "\n",
    "        lm_outputs, cluster_outputs = model(texts=texts)\n",
    "        \n",
    "        batch_embs.append(cluster_outputs.embeddings.numpy())\n",
    "\n",
    "        combined_loss = lm_outputs.loss + 0.025 * cluster_outputs.loss\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        pbar.set_description(f'Epoch: {epoch} | LM Loss: {lm_outputs.loss.item()} | Cluster Loss: {cluster_outputs.loss.item()}')\n",
    "        \n",
    "        true_labels.extend(labels.numpy().astype('int'))\n",
    "        predicted_labels.extend(cluster_outputs.predicted_labels.numpy().astype('int'))\n",
    "    \n",
    "    emb_hist.append(np.vstack(batch_embs))\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    print(f'Epoch: {epoch} | Cluster acc: {cluster_accuracy(true_labels, predicted_labels)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "reducer = UMAP(n_components=2)\n",
    "Xr = reducer.fit_transform(emb_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=Xr[:,0], y=Xr[:,1], hue=[f'C{i}' for i in predicted_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_hist[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
