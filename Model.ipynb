{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a test bench for working on the core model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keller/.conda/envs/cuda/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.cluster.k_means_ import _k_init\n",
    "from sklearn.utils.extmath import row_norms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def cluster_accuracy(y_true, y_predicted, cluster_number: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy after using the linear_sum_assignment function in SciPy to\n",
    "    determine reassignments.\n",
    "    :param y_true: list of true cluster numbers, an integer array 0-indexed\n",
    "    :param y_predicted: list  of predicted cluster numbers, an integer array 0-indexed\n",
    "    :param cluster_number: number of clusters, if None then calculated from input\n",
    "    :return: reassignment dictionary, clustering accuracy\n",
    "    \"\"\"\n",
    "    if cluster_number is None:\n",
    "        cluster_number = (\n",
    "            max(y_predicted.max(), y_true.max()) + 1\n",
    "        )  # assume labels are 0-indexed\n",
    "    count_matrix = np.zeros((cluster_number, cluster_number), dtype=np.int64)\n",
    "    for i in range(y_predicted.size):\n",
    "        count_matrix[y_predicted[i], y_true[i]] += 1\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(count_matrix.max() - count_matrix)\n",
    "    reassignment = dict(zip(row_ind, col_ind))\n",
    "    accuracy = count_matrix[row_ind, col_ind].sum() / y_predicted.size\n",
    "    return reassignment, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.texts[index], self.labels[index]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_distance(X, Y, p=2):\n",
    "    \"\"\"\n",
    "    Computes row wise minkowski distances between matrices X and Y\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.abs(X-Y)**p, dim=1)**(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(X, Y):\n",
    "    \"\"\"\n",
    "    Computes the rowwise cosine distance between matrices X and Y.\n",
    "    \"\"\"\n",
    "    dots = torch.stack([torch.dot(xi, yi) for xi, yi in zip(X, Y)])\n",
    "    norms = torch.stack([torch.dot(xi.unsqueeze(0), yi.unsqueeze(0)) for xi, yi in zip(torch.norm(X, dim=1), torch.norm(Y, dim=1))])\n",
    "    cos_sim = torch.div(dots, norms)\n",
    "    return 1.0 - cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import *\n",
    "\n",
    "def mask_tokens(inputs: torch.Tensor, tokenizer) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "    \"\"\"\n",
    "\n",
    "    if tokenizer.mask_token is None:\n",
    "        raise ValueError(\n",
    "            \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
    "        )\n",
    "\n",
    "    labels = inputs.clone()\n",
    "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "    probability_matrix = torch.full(labels.shape, 0.15)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    if tokenizer._pad_token is not None:\n",
    "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sparsity(X):\n",
    "    \"\"\"\n",
    "    Computes the mean of the row-wise sparsity of the input feature-matrix X\n",
    "    \"\"\"\n",
    "    return np.mean(np.count_nonzero(X, axis=1) / X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "@dataclass\n",
    "class ClusterOutput(ModelOutput):\n",
    "    \n",
    "    loss: torch.FloatTensor = None\n",
    "    predicted_labels: torch.IntTensor = None\n",
    "    embeddings: torch.FloatTensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_embedding_extractor(model_output: ModelOutput):\n",
    "    return model_output.last_hidden_state[:, 0, :].float()\n",
    "\n",
    "def meanpooler_embedding_extractor(model_ouput: ModelOutput):\n",
    "    return model_ouput.last_hidden_state.mean(dim=1).float()\n",
    "\n",
    "def concat_cls_n_hidden_states(model_output: ModelOutput, n=3):\n",
    "    n_hidden_states = model_output.hidden_states[-n:]\n",
    "    return torch.cat([t[:, 0, :] for t in n_hidden_states], 1).float()\n",
    "\n",
    "def concat_mean_n_hidden_states(model_output: ModelOutput, n=3):\n",
    "    n_hidden_states = model_output.hidden_states[-n:]\n",
    "    return torch.cat([t.mean(dim=1) for t in n_hidden_states], 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterLM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 initial_centroids: torch.tensor,\n",
    "                 lm_model,\n",
    "                 tokenizer,\n",
    "                 metric=lp_distance,\n",
    "                 embedding_extractor=cls_embedding_extractor,\n",
    "                 do_language_modeling=True,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "        super(ClusterLM, self).__init__()\n",
    "\n",
    "        self.initial_centroids = initial_centroids\n",
    "\n",
    "        self.add_module('lm_model', lm_model)\n",
    "        self.register_parameter('centroids', nn.Parameter(initial_centroids.clone().float(), requires_grad=True))\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.metric = metric\n",
    "        self.embedding_extractor = embedding_extractor\n",
    "        self.do_language_modeling = do_language_modeling\n",
    "        self.device = device\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, texts, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Input: texts and labels (optional)\n",
    "        Returns: lm_language modelling output, own output dict (clustering_loss, predicted_labels)\n",
    "        \"\"\"\n",
    "        # Language Modeling Part:\n",
    "\n",
    "        lm_outputs = ModelOutput(loss=torch.tensor(0.0, requires_grad=True).to(self.device))\n",
    "\n",
    "        if self.do_language_modeling:\n",
    "            inputs = self.tokenizer(\n",
    "                texts,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                truncation=True)\n",
    "\n",
    "            input_ids = inputs['input_ids'].clone()\n",
    "            input_ids, true_ids = mask_tokens(input_ids, self.tokenizer)\n",
    "            inputs['input_ids'] = input_ids\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            true_ids = true_ids.to(self.device)\n",
    "            lm_outputs = self.lm_model(labels=true_ids, **inputs)\n",
    "\n",
    "        # Clustering Part:\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True)\n",
    "\n",
    "        inputs.to(self.device)\n",
    "\n",
    "        # 0. Obtain embeddings for each input\n",
    "        input_embeddings = self.embedding_extractor(self.lm_model.base_model(**inputs))\n",
    "        \n",
    "        # 1. Compute distances from each input embedding to each centroids\n",
    "        distances = torch.stack([self.metric(embedding.unsqueeze(0), self.centroids) for embedding in input_embeddings])\n",
    "        nearest_centroids = torch.argmin(distances.cpu().clone().detach(), dim=1)\n",
    "        distances = torch.transpose(distances, 0, 1)  # => shape (n_centroids, n_samples)\n",
    "\n",
    "        # 2. Compute the paramterized softmin for each centroid of each distance to each centroid per input sample\n",
    "        # Find min distances for each centroid\n",
    "        min_distances = torch.min(distances, dim=1).values\n",
    "        # Compute exponetials\n",
    "        exponentials = torch.exp(- alpha * (distances - min_distances.unsqueeze(1)))\n",
    "        # Compute softmin\n",
    "        softmin = exponentials / torch.sum(exponentials, dim=1).unsqueeze(1)\n",
    "\n",
    "        # 3. Weight the distance between each sample and each centroid\n",
    "        weighted_distances = distances * softmin\n",
    "\n",
    "        # 4. Sum over weighted_distances to obtain loss\n",
    "        clustering_loss = weighted_distances.sum(dim=1).mean()\n",
    "\n",
    "        # Create clustering output dictionary\n",
    "        cluster_outputs = ClusterOutput(\n",
    "            loss=clustering_loss,\n",
    "            predicted_labels=nearest_centroids.long(),\n",
    "            embeddings=input_embeddings.cpu().detach()\n",
    "        )\n",
    "\n",
    "        return lm_outputs, cluster_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(\n",
    "    lm_model,\n",
    "    tokenizer,\n",
    "    data_loader,\n",
    "    n_clusters,\n",
    "    embedding_extractor=concat_cls_n_hidden_states,\n",
    "    device='cpu',\n",
    "    random_state=np.random.RandomState(42),\n",
    "    **kwargs,\n",
    "):\n",
    "    \n",
    "    initial_embeddings = []\n",
    "    for batch_texts, _ in data_loader:\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = lm_model.base_model(**inputs)\n",
    "        extracted_embeddings = embedding_extractor(outputs).cpu().detach().numpy()\n",
    "        initial_embeddings.append(extracted_embeddings)\n",
    "        \n",
    "    initial_embeddings = np.vstack(initial_embeddings)\n",
    "    \n",
    "    \n",
    "    initial_centroids = _k_init(\n",
    "        initial_embeddings,\n",
    "        n_clusters=np.unique(labels).shape[0],\n",
    "        x_squared_norms=row_norms(initial_embeddings, squared=True),\n",
    "        random_state=np.random.RandomState(42)\n",
    "    )\n",
    "    \n",
    "    model = ClusterLM(\n",
    "        lm_model=lm_model,\n",
    "        tokenizer=tokenizer,\n",
    "        embedding_extractor=embedding_extractor,\n",
    "        initial_centroids=torch.from_numpy(initial_centroids),\n",
    "        device=device,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return model, initial_centroids, initial_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_data_loader, verbose=True):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    eval_data_it = tqdm(eval_data_loader, desc='Eval') if verbose else eval_data_loader\n",
    "    for batch_texts, batch_labels in eval_data_it:\n",
    "        true_labels.extend(batch_labels.numpy().astype('int'))\n",
    "        _, cluster_outputs = model(texts=list(batch_texts))\n",
    "        predicted_labels.extend(cluster_outputs.predicted_labels.numpy().astype('int'))\n",
    "    \n",
    "    return np.array(predicted_labels), np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainHistory:\n",
    "    clustering_losses: List[float] \n",
    "    lm_losses: List[float]\n",
    "    combined_losses: List[float]\n",
    "    prediction_history: List[np.array]\n",
    "    eval_hist: List[Dict[str, float]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    n_epochs,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    annealing_alphas,\n",
    "    train_data_loader,\n",
    "    eval_data_loader=None,\n",
    "    do_eval=True,\n",
    "    clustering_loss_weight=0.5,\n",
    "    metrics=(cluster_accuracy, adjusted_rand_score, normalized_mutual_info_score),\n",
    "    verbose=True\n",
    "):\n",
    "    \n",
    "    total_clustering_losses = []\n",
    "    total_lm_losses = []\n",
    "    total_combined_losses = []\n",
    "    prediction_history = []\n",
    "    eval_hist = []\n",
    "    \n",
    "    assert len(annealing_alphas) >= n_epochs\n",
    "    for epoch, alpha in zip(range(n_epochs), annealing_alphas):\n",
    "        model.train()\n",
    "        train_data_it = tqdm(train_data_loader, desc='Train') if verbose else train_data_loader\n",
    "        for batch_texts, _ in train_data_it:\n",
    "            lm_outputs, cluster_outputs = model(texts=list(batch_texts), alpha=alpha)\n",
    "            combined_loss = lm_outputs.loss + (clustering_loss_weight * cluster_outputs.loss)\n",
    "            \n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_clustering_losses.append(cluster_outputs.loss.item())\n",
    "            total_lm_losses.append(lm_outputs.loss.item())\n",
    "            total_combined_losses.append(combined_loss.item())\n",
    "            \n",
    "            if verbose:\n",
    "                train_data_it.set_description(\n",
    "                    f'Epoch: {epoch} | CombLoss: {combined_loss.item()} |LMLoss: {lm_outputs.loss.item()} | ' \\\n",
    "                    f' ClusterLoss: {cluster_outputs.loss.item()} | LR: {scheduler.get_last_lr()[0]} | Alpha: {alpha}'\n",
    "                )\n",
    "            \n",
    "            \n",
    "        if do_eval:\n",
    "            if eval_data_loader is None:\n",
    "                eval_data_loader = train_data_it if not verbose else train_data_it.iterable\n",
    "            \n",
    "            predicted_labels, true_labels = evaluate(\n",
    "                model=model,\n",
    "                eval_data_loader=eval_data_loader,\n",
    "                verbose=verbose)\n",
    "            \n",
    "            prediction_history.append(deepcopy(predicted_labels))\n",
    "            \n",
    "            m = {}\n",
    "            for metric in metrics:\n",
    "                value = metric(true_labels, predicted_labels)\n",
    "                m[metric.__name__] = value\n",
    "                print(f'{metric.__name__}: {value}')\n",
    "            eval_hist.append(m)\n",
    "    \n",
    "    train_history = TrainHistory(\n",
    "        clustering_losses=total_clustering_losses,\n",
    "        lm_losses=total_lm_losses,\n",
    "        combined_losses=total_combined_losses,\n",
    "        prediction_history=prediction_history,\n",
    "        eval_hist=eval_hist\n",
    "    )\n",
    "    return train_history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test params \n",
    "N_EPOCHS = 8\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "EMBEDDING_EXTRACTOR = concat_cls_n_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/ag_news_subset5.csv')\n",
    "\n",
    "texts = df['texts'].to_numpy()\n",
    "labels = df['labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_datasets as tfds\n",
    "#from itertools import chain\n",
    "#\n",
    "#train_ds = tfds.load('ag_news_subset', split='train', shuffle_files=True)\n",
    "#test_ds = tfds.load('ag_news_subset', split='test', shuffle_files=True)\n",
    "#\n",
    "#texts, labels = [], []\n",
    "#\n",
    "#for ds in (train_ds, test_ds):\n",
    "#    for example in tfds.as_numpy(ds):\n",
    "#        text, label = example['description'], example['label']\n",
    "#        texts.append(text.decode(\"utf-8\"))\n",
    "#        labels.append(label)\n",
    "#\n",
    "#labels = np.array(labels)\n",
    "#\n",
    "#del train_ds\n",
    "#del test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#texts, _, labels, _ = train_test_split(texts, labels, stratify=labels, test_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataset(texts, labels)\n",
    "data_loader = DataLoader(dataset=data, batch_size=16, shuffle=False) # batch size bigger => results better\n",
    "# 16 for distilbert | 8 for bert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "from transformers import ElectraTokenizer, ElectraForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "lm_model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "#lm_model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', return_dict=True, output_hidden_states=True)\n",
    "#lm_model = BertForMaskedLM.from_pretrained('bert-large-uncased', return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base', return_dict=True, output_hidden_states=True)\n",
    "#lm_model = RobertaForMaskedLM.from_pretrained('roberta-base', return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator', return_dict=True, output_hidden_states=True)\n",
    "#lm_model = ElectraForMaskedLM.from_pretrained('google/electra-base-discriminator', return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\", return_dict=True, output_hidden_states=True)\n",
    "#lm_model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\", return_dict=True, output_hidden_states=True)\n",
    "\n",
    "\n",
    "#tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\", return_dict=True, output_hidden_states=True)\n",
    "#lm_model = XLNetLMHeadModel.from_pretrained(\"xlnet-base-cased\", return_dict=True, mem_len=1024, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"textattack/distilbert-base-uncased-STS-B\", return_dict=True, output_hidden_states=True)\n",
    "#lm_model = AutoModelForMaskedLM.from_pretrained(\"textattack/distilbert-base-uncased-STS-B\", return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-cased-STS-B\", return_dict=True, output_hidden_states=True)\n",
    "#lm_model = AutoModelForMaskedLM.from_pretrained(\"textattack/bert-base-cased-STS-B\", return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-ag-news\")\n",
    "#lm_model = AutoModelForMaskedLM.from_pretrained(\"textattack/bert-base-uncased-ag-news\", return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"allenai/news_roberta_base\")\n",
    "#lm_model = AutoModelForMaskedLM.from_pretrained(\"allenai/news_roberta_base\", return_dict=True, output_hidden_states=True)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
    "#lm_model = AutoModelForMaskedLM.from_pretrained(\"prajjwal1/bert-small\", return_dict=True, output_hidden_states=True)\n",
    "\n",
    "\n",
    "lm_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_value = 1  # specs.embedding_size # Used to modify the range of the alpha scheme\n",
    "alphas = np.zeros(N_EPOCHS, dtype=float)\n",
    "alphas[0] = 0.1\n",
    "for i in range(1, N_EPOCHS):\n",
    "    alphas[i] = (2 ** (1 / (np.log(i + 1)) ** 2)) * alphas[i - 1]\n",
    "annealing_alphas = alphas / constant_value\n",
    "#annealing_alphas = np.ones(N_EPOCHS) * 1000.0\n",
    "annealing_alphas = np.arange(1, N_EPOCHS+1).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, initial_centroids, initial_embeddings = init_model(\n",
    "    lm_model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_loader=data_loader,\n",
    "    embedding_extractor=EMBEDDING_EXTRACTOR,\n",
    "    n_clusters=np.unique(labels).shape[0],\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# RMSprop\n",
    "# or Adam(W)???\n",
    "opt = torch.optim.RMSprop(\n",
    "    params=model.parameters(),\n",
    "    lr=2e-5, #2e-5, 5e-7, \n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "total_steps = len(data_loader) * N_EPOCHS\n",
    "\n",
    "#scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "#    optimizer=opt,\n",
    "#    num_warmup_steps=int(len(data_loader)*0.5),\n",
    "#    num_training_steps=total_steps,\n",
    "#    num_cycles=N_EPOCHS//2\n",
    "#)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=opt,\n",
    "    num_warmup_steps=int(len(data_loader)*0.5),\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | CombLoss: 5.398853302001953 |LMLoss: 2.5615742206573486 | ClusterLoss: 5.674558639526367 | LR: 1.866354827931841e-05 | Alpha: 1.0: 100%|██████████| 399/399 [01:03<00:00,  6.26it/s]   \n",
      "Eval: 100%|██████████| 399/399 [00:22<00:00, 17.51it/s]\n",
      "Epoch: 1 | CombLoss: 5.509673118591309 |LMLoss: 2.43392014503479 | ClusterLoss: 6.151505470275879 | LR: 1.8656866020715002e-05 | Alpha: 2.0:   0%|          | 1/399 [00:00<01:13,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_accuracy: ({0: 1, 1: 3, 2: 2, 3: 0}, 0.7136363636363636)\n",
      "adjusted_rand_score: 0.4142221310901126\n",
      "normalized_mutual_info_score: 0.42986606091674817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | CombLoss: 3.63856840133667 |LMLoss: 1.892756462097168 | ClusterLoss: 3.491623878479004 | LR: 1.599732709655864e-05 | Alpha: 2.0: 100%|██████████| 399/399 [01:04<00:00,  6.19it/s]     \n",
      "Eval: 100%|██████████| 399/399 [00:22<00:00, 17.52it/s]\n",
      "Epoch: 2 | CombLoss: 4.274910926818848 |LMLoss: 2.370936870574951 | ClusterLoss: 3.807947874069214 | LR: 1.599064483795523e-05 | Alpha: 3.0:   0%|          | 1/399 [00:00<01:14,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_accuracy: ({0: 1, 1: 3, 2: 2, 3: 0}, 0.7653605015673981)\n",
      "adjusted_rand_score: 0.5098268031197889\n",
      "normalized_mutual_info_score: 0.5174567107385716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | CombLoss: 3.874776840209961 |LMLoss: 2.409452199935913 | ClusterLoss: 2.9306490421295166 | LR: 1.3331105913798865e-05 | Alpha: 3.0: 100%|██████████| 399/399 [01:05<00:00,  6.14it/s]  \n",
      "Eval: 100%|██████████| 399/399 [00:23<00:00, 17.33it/s]\n",
      "Epoch: 3 | CombLoss: 3.325136661529541 |LMLoss: 1.7496110200881958 | ClusterLoss: 3.1510510444641113 | LR: 1.3324423655195457e-05 | Alpha: 4.0:   0%|          | 1/399 [00:00<01:14,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_accuracy: ({0: 1, 1: 3, 2: 2, 3: 0}, 0.7735109717868338)\n",
      "adjusted_rand_score: 0.5288778757652373\n",
      "normalized_mutual_info_score: 0.5282231438807025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | CombLoss: 3.5029137134552 |LMLoss: 2.167978525161743 | ClusterLoss: 2.669870376586914 | LR: 1.0664884731039091e-05 | Alpha: 4.0: 100%|██████████| 399/399 [01:04<00:00,  6.19it/s]     \n",
      "Eval: 100%|██████████| 399/399 [00:22<00:00, 17.63it/s]\n",
      "Epoch: 4 | CombLoss: 3.0954971313476562 |LMLoss: 1.8931844234466553 | ClusterLoss: 2.404625415802002 | LR: 1.0658202472435684e-05 | Alpha: 5.0:   0%|          | 1/399 [00:00<01:14,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_accuracy: ({0: 1, 1: 3, 2: 2, 3: 0}, 0.7785266457680251)\n",
      "adjusted_rand_score: 0.5363923795688491\n",
      "normalized_mutual_info_score: 0.5317209009756211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | CombLoss: 3.733665943145752 |LMLoss: 2.4775240421295166 | ClusterLoss: 2.5122838020324707 | LR: 1.0163715335783494e-05 | Alpha: 5.0:  19%|█▉        | 75/399 [00:12<00:52,  6.17it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c9d67af7529f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-01009bc95bda>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, model, optimizer, scheduler, annealing_alphas, train_data_loader, eval_data_loader, do_eval, clustering_loss_weight, metrics, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mcombined_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = train(\n",
    "    n_epochs=N_EPOCHS,\n",
    "    model=model,\n",
    "    optimizer=opt,\n",
    "    scheduler=scheduler,\n",
    "    annealing_alphas=annealing_alphas,\n",
    "    train_data_loader=data_loader,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
