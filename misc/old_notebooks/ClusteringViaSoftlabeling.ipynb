{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import time\n",
    "torch.cuda.is_available(), torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusteringLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, initial_centroids, alpha=1.0):\n",
    "        super(ClusteringLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(initial_centroids)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes a batch of input embeddings of shape (batch_size, embedding_size).\n",
    "        And computes of soft clustering based in on the centroids.\n",
    "        \"\"\"\n",
    "        \n",
    "        #linear_projection = nn.Linear(in_features=768, out_features=10)\n",
    "        #linear_projection = linear_projection.double()\n",
    "        #linear_projection = linear_projection.to('cuda')\n",
    "        #\n",
    "        #tanh = nn.Tanh()\n",
    "        #tanh = tanh.double()\n",
    "        #tanh = tanh.to('cuda')\n",
    "        \n",
    "        #weights = nn.Parameter(tanh(linear_projection(self.weights)))\n",
    "        \n",
    "        \n",
    "        q = 1.0 / (1.0 + (torch.sum(torch.pow(\n",
    "            torch.unsqueeze(inputs, 1) - self.weights, exponent=2), dim=2) / self.alpha))\n",
    "        q = torch.pow(q, exponent=(self.alpha + 1.0) / 2.0)\n",
    "        q = torch.transpose(torch.transpose(q, 0, 1) / torch.sum(q, dim=1), 0, 1)\n",
    "        return q\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.weights.shape)\n",
    "\n",
    "    \n",
    "class DistilBertForClustering(nn.Module):\n",
    "    \n",
    "    def __init__(self, distilbert_model, initial_centroids):\n",
    "        super(DistilBertForClustering, self).__init__()\n",
    "        self.distilbert_model = distilbert_model\n",
    "        self.initial_centroids = initial_centroids\n",
    "        self.clustering_layer = ClusteringLayer(initial_centroids=initial_centroids)\n",
    "        \n",
    "    def target_probability_distribution(self, q):\n",
    "        p = q ** 2 / q.sum(0)\n",
    "        p = p / p.sum(dim=1, keepdim=True)\n",
    "        return p\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        distilbert_outputs = self.distilbert_model(**inputs)\n",
    "        cls_embeddings = distilbert_outputs.last_hidden_state[:,0,:]\n",
    "        \n",
    "        #cls_embeddings = cls_embeddings.double()\n",
    "        #linear_projection = nn.Linear(in_features=768, out_features=10)\n",
    "        #linear_projection = linear_projection.double()\n",
    "        #linear_projection = linear_projection.to('cuda')\n",
    "        #tanh = nn.Tanh()\n",
    "        #tanh = tanh.double()\n",
    "        #tanh = tanh.to('cuda')\n",
    "        \n",
    "        #cls_embeddings = linear_projection(cls_embeddings)\n",
    "        #cls_embeddings = tanh(cls_embeddings)\n",
    "        \n",
    "        q = self.clustering_layer(cls_embeddings)\n",
    "        p = self.target_probability_distribution(q)\n",
    "        return q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusteringLayer2(nn.Module):\n",
    "    \n",
    "    def __init__(self, initial_centroids, alpha=1.0):\n",
    "        super(ClusteringLayer2, self).__init__()\n",
    "        self.weights = nn.Parameter(initial_centroids)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes a batch of input embeddings of shape (batch_size, embedding_size).\n",
    "        And computes of soft clustering based in on the centroids.\n",
    "        \"\"\"\n",
    "        \n",
    "        #linear_projection = nn.Linear(in_features=768, out_features=10)\n",
    "        #linear_projection = linear_projection.double()\n",
    "        #linear_projection = linear_projection.to('cuda')\n",
    "        #\n",
    "        #tanh = nn.Tanh()\n",
    "        #tanh = tanh.double()\n",
    "        #tanh = tanh.to('cuda')\n",
    "        \n",
    "        #weights = nn.Parameter(tanh(linear_projection(self.weights)))\n",
    "        \n",
    "        \n",
    "        norm_squared = torch.sum((inputs.unsqueeze(1) - self.weights) ** 2, 2)\n",
    "        numerator = 1.0 / (1.0 + (norm_squared / self.alpha))\n",
    "        power = float(self.alpha + 1) / 2\n",
    "        q = numerator ** power\n",
    "        return q\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.weights.shape)\n",
    "\n",
    "    \n",
    "class DistilBertForClustering2(nn.Module):\n",
    "    \n",
    "    def __init__(self, distilbert_model, initial_centroids):\n",
    "        super(DistilBertForClustering2, self).__init__()\n",
    "        self.distilbert_model = distilbert_model\n",
    "        self.initial_centroids = initial_centroids\n",
    "        self.clustering_layer = ClusteringLayer2(initial_centroids=initial_centroids)\n",
    "        \n",
    "    def target_probability_distribution(self, q):\n",
    "        weight = (q ** 2) / torch.sum(q, 0)\n",
    "        return (weight.t() / torch.sum(weight, 1)).t()\n",
    "    \n",
    "    #def target_probability_distribution(self, q):\n",
    "    #    a = q.argmax(1)\n",
    "    #    a = a.to('cuda')\n",
    "    #    p = torch.zeros(q.shape)\n",
    "    #    p = p.to('cuda')\n",
    "    #    p = p.scatter(1, a.unsqueeze(1), 1.0)\n",
    "    #    p = p.to('cuda')\n",
    "    #    p = p.double()\n",
    "    #    return p\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        distilbert_outputs = self.distilbert_model(**inputs)\n",
    "        cls_embeddings = distilbert_outputs.last_hidden_state[:,0,:]\n",
    "        \n",
    "        #cls_embeddings = cls_embeddings.double()\n",
    "        #linear_projection = nn.Linear(in_features=768, out_features=10)\n",
    "        #linear_projection = linear_projection.double()\n",
    "        #linear_projection = linear_projection.to('cuda')\n",
    "        #tanh = nn.Tanh()\n",
    "        #tanh = tanh.double()\n",
    "        #tanh = tanh.to('cuda')\n",
    "        \n",
    "        #cls_embeddings = linear_projection(cls_embeddings)\n",
    "        #cls_embeddings = tanh(cls_embeddings)\n",
    "        \n",
    "        q = self.clustering_layer(cls_embeddings)\n",
    "        p = self.target_probability_distribution(q)\n",
    "        return q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def cluster_accuracy(y_true, y_predicted, cluster_number: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy after using the linear_sum_assignment function in SciPy to\n",
    "    determine reassignments.\n",
    "    :param y_true: list of true cluster numbers, an integer array 0-indexed\n",
    "    :param y_predicted: list  of predicted cluster numbers, an integer array 0-indexed\n",
    "    :param cluster_number: number of clusters, if None then calculated from input\n",
    "    :return: reassignment dictionary, clustering accuracy\n",
    "    \"\"\"\n",
    "    if cluster_number is None:\n",
    "        cluster_number = (\n",
    "            max(y_predicted.max(), y_true.max()) + 1\n",
    "        )  # assume labels are 0-indexed\n",
    "    count_matrix = np.zeros((cluster_number, cluster_number), dtype=np.int64)\n",
    "    for i in range(y_predicted.size):\n",
    "        count_matrix[y_predicted[i], y_true[i]] += 1\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(count_matrix.max() - count_matrix)\n",
    "    reassignment = dict(zip(row_ind, col_ind))\n",
    "    accuracy = count_matrix[row_ind, col_ind].sum() / y_predicted.size\n",
    "    return reassignment, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "distilbert_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "data = fetch_20newsgroups(\n",
    "    categories=['comp.graphics', 'talk.religion.misc', 'rec.autos', 'sci.med'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "texts, labels = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stop_words import get_stop_words\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "#import re\n",
    "#import spacy\n",
    "#\n",
    "#\n",
    "## create syntethic texts\n",
    "#data = fetch_20newsgroups(\n",
    "#    categories=['comp.graphics', 'talk.religion.misc'],\n",
    "#    remove=('headers', 'footers', 'quotes')\n",
    "#)\n",
    "#texts, labels = data.data , data.target\n",
    "#\n",
    "#cleaned_texts = []\n",
    "#stopwords = get_stop_words('en')\n",
    "#\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#\n",
    "#for text in texts:\n",
    "#    #tokens = [token for token in re.findall(r'\\w+', text) if token not in stopwords]\n",
    "#    \n",
    "#    doc = nlp(text)\n",
    "#    noun_phrases = [str(np) for np in doc.noun_chunks]\n",
    "#    \n",
    "#    cleaned_texts.append(\" \".join(noun_phrases))\n",
    "#texts = cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "under_sampler = RandomUnderSampler()\n",
    "texts, labels = under_sampler.fit_resample([[t] for t in texts], labels)\n",
    "texts = [t[0] for t in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_datasets as tfds\n",
    "#from itertools import chain\n",
    "#\n",
    "#train_ds = tfds.load('imdb_reviews', split='train', shuffle_files=True)\n",
    "#test_ds = tfds.load('imdb_reviews', split='test', shuffle_files=True)\n",
    "#\n",
    "#texts, labels = [], []\n",
    "#\n",
    "#for ds in (train_ds, test_ds):\n",
    "#    for example in tfds.as_numpy(ds):\n",
    "#        text, label = example['text'], example['label']\n",
    "#        texts.append(str(text))\n",
    "#        labels.append(label)\n",
    "#\n",
    "#labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#texts, _, labels, _ = train_test_split(texts, labels, test_size=0.75)\n",
    "#len(texts), np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=, random_state=42)\n",
    "train_texts, train_labels = texts, labels\n",
    "test_texts, test_labels = [\"DUMMY\"], [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "embeddings = []\n",
    "for index, text in tqdm(enumerate(train_texts)):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to('cuda')\n",
    "    outputs = distilbert_model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:,0,:].flatten().cpu().detach().numpy()\n",
    "    embeddings.append(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=np.unique(labels).shape[0], n_init=20)\n",
    "\n",
    "kmeans_cluster = kmeans.fit_predict(embeddings)\n",
    "kmeans_centroids = torch.from_numpy(kmeans.cluster_centers_)\n",
    "kmeans_centroids = kmeans_centroids.to('cuda')\n",
    "kmeans_centroids, kmeans_centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "print(cluster_accuracy(train_labels, kmeans_cluster)[1])\n",
    "print(normalized_mutual_info_score(train_labels, kmeans_cluster))\n",
    "print(adjusted_rand_score(train_labels, kmeans_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = DistilBertForClustering2(distilbert_model=distilbert_model, initial_centroids=kmeans_centroids)\n",
    "cluster_model.distilbert_model.requires_grad_(True)\n",
    "cluster_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.texts[index], labels[index]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TextDataset(train_texts, train_labels, tokenizer=tokenizer)\n",
    "test_data = TextDataset(test_texts, test_labels, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.SGD(params=cluster_model.parameters(), lr=1e-03)\n",
    "#optimizer = torch.optim.SGD(params=cluster_model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.AdamW(params=cluster_model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "def kld(target, pred):\n",
    "            return torch.mean(torch.sum(target*torch.log(target/(pred+1e-6)), dim=1))\n",
    "    \n",
    "def kl_divergence(target, pred):\n",
    "    return torch.mean(torch.sum(target * torch.log(target/pred), dim=1))\n",
    "\n",
    "loss_fn = nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    cluster_model.train()  # sets the model intr o trainnode => Some Layers like Normalization or Dropout are activated\n",
    "    for param in cluster_model.distilbert_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    train_pbar = tqdm(enumerate(train_loader))\n",
    "    for batch_index, batch_data in train_pbar:\n",
    "        texts, _ = batch_data\n",
    "        inputs = tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        inputs = inputs.to('cuda')\n",
    "        q, p = cluster_model(inputs)\n",
    "        #loss = kl_divergence(p, q)\n",
    "        #loss = kld(p, q)\n",
    "        loss = loss_fn(q.log(), p)\n",
    "        optimizer.zero_grad()  # clears the gradients (from previous optimization step)\n",
    "        loss.backward()  # backpropagation step through the model\n",
    "        optimizer.step()  # updated the weights of each layer using the computed gradients\n",
    "        train_pbar.set_description(f'Epoch {epoch + 1} | Loss {loss.item()}')\n",
    "    \n",
    "    predicted_labels_total = []\n",
    "    with torch.no_grad():\n",
    "        cluster_model.eval()\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        test_pbar = tqdm(enumerate(train_loader))\n",
    "        for batch_index, batch_data in test_pbar:\n",
    "            texts, labels = batch_data\n",
    "            inputs = tokenizer(\n",
    "                texts,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            inputs = inputs.to('cuda')\n",
    "            q, p = cluster_model(inputs)\n",
    "            \n",
    "            predicted_label = q.argmax(dim=1).cpu().detach().numpy()\n",
    "            true_label = labels.cpu().detach().numpy()\n",
    "            \n",
    "            predicted_labels.extend(predicted_label)\n",
    "            true_labels.extend(true_label)\n",
    "            \n",
    "        true_labels = np.array(true_labels).flatten()\n",
    "        predicted_labels = np.array(predicted_labels).flatten()\n",
    "        predicted_labels_total.extend(predicted_labels)\n",
    "\n",
    "        print('#'*60)\n",
    "        print(normalized_mutual_info_score(true_labels, predicted_labels))\n",
    "        print(cluster_accuracy(true_labels, predicted_labels))\n",
    "        print(adjusted_rand_score(true_labels, predicted_labels))\n",
    "        return np.array(predicted_labels_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_params = np.array([param.cpu().detach().numpy() for param in distilbert_model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for epoch in range(5):\n",
    "    predicted_labels = train(epoch)\n",
    "    predictions.append(train(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cluster_model, f\"cluster_model_{time.time()}.bin\")\n",
    "torch.save(distilbert_model, f\"distilbert_model_{time.time()}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tuned_embeddings = []\n",
    "for index, text in tqdm(enumerate(train_texts)):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to('cuda')\n",
    "    outputs = distilbert_model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:,0,:].flatten().cpu().detach().numpy()\n",
    "    tuned_embeddings.append(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "Xr = pca.fit_transform(embeddings)\n",
    "Xrt = pca.fit_transform(tuned_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "untuned_plot = sns.scatterplot(x=Xr[:,0], y=Xr[:,1], hue=[f'C{i}' for i in predicted_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_plot.get_figure().savefig(f'untuned_plot_{time.time()}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_plot = sns.scatterplot(x=Xrt[:,0], y=Xrt[:,1], hue=[f'C{i}' for i in predicted_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_plot.get_figure().savefig(f'tuned_plot_{time.time()}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_params = np.array([param.cpu().detach().numpy() for param in distilbert_model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_params[0] == end_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_params[0][start_params[0] == end_params[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model.distilbert_model.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[predicted_labels != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "print(texts[index])\n",
    "inputs = tokenizer(texts[index], return_tensors='pt', padding=True, truncation=True)\n",
    "inputs = inputs.to('cuda')\n",
    "q, p = cluster_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(q.log(), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, euclidean, squareform\n",
    "\n",
    "distances = squareform(pdist(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[0, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_classification, make_circles\n",
    "\n",
    "X, y = make_classification(n_samples=100000, n_classes=3, n_clusters_per_class=3, n_features=20, n_informative=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans_pred = kmeans.fit_predict(X)\n",
    "\n",
    "kmeans_centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_accuracy(y, kmeans_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "Xr = pca.fit_transform(X)\n",
    "sns.scatterplot(x=Xr[:,0], y=Xr[:,1], hue=[f'C{i}' for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clusteringlayer(X, y, clustering_layer, optimizer, loss_fn):\n",
    "    \n",
    "    \n",
    "    #X = torch.from_numpy(X)\n",
    "    \n",
    "    \n",
    "    def target_probability_distribution(q):\n",
    "        weight = (q ** 2) / torch.sum(q, 0)\n",
    "        return (weight.t() / torch.sum(weight, 1)).t()\n",
    "    \n",
    "    \n",
    "    clustering_layer = clustering_layer.train()\n",
    "    q = clustering_layer(X)\n",
    "    \n",
    "    p = target_probability_distribution(q)\n",
    "    \n",
    "    loss = loss_fn(q.log(), p)\n",
    "    \n",
    "    optimizer.zero_grad()  # clears the gradients (from previous optimization step)\n",
    "    \n",
    "    loss.backward()  # backpropagation step through the model\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    predicted_label = q.argmax(dim=1).cpu().detach().numpy()\n",
    "    _, accuracy = cluster_accuracy(y, predicted_label)\n",
    "    #print(f'Loss: {loss.item()}')\n",
    "    #print(f'Accuracy: {}')\n",
    "    \n",
    "    return p, q, loss.item(), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusteringLayer3(nn.Module):\n",
    "    \n",
    "    def __init__(self, initial_centroids, alpha=1.0, in_features=768, hidden_dim=100):\n",
    "        super(ClusteringLayer3, self).__init__()\n",
    "        self.linear_layer = nn.Linear(in_features, hidden_dim).double()\n",
    "        self.weights = nn.Parameter(initial_centroids)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes a batch of input embeddings of shape (batch_size, embedding_size).\n",
    "        And computes of soft clustering based in on the centroids.\n",
    "        \"\"\"\n",
    "        \n",
    "        #linear_projection = nn.Linear(in_features=768, out_features=10)\n",
    "        #linear_projection = linear_projection.double()\n",
    "        #linear_projection = linear_projection.to('cuda')\n",
    "        #\n",
    "        #tanh = nn.Tanh()\n",
    "        #tanh = tanh.double()\n",
    "        #tanh = tanh.to('cuda')\n",
    "        \n",
    "        #weights = nn.Parameter(tanh(linear_projection(self.weights)))\n",
    "        \n",
    "        embedded_weights = self.linear_layer(self.weights)\n",
    "        \n",
    "        norm_squared = torch.sum((inputs.unsqueeze(1) - embedded_weights) ** 2, 2)\n",
    "        numerator = 1.0 / (1.0 + (norm_squared / self.alpha))\n",
    "        power = float(self.alpha + 1) / 2\n",
    "        q = numerator ** power\n",
    "        return q\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, initial_centroids, in_features=768, hidden_dim=100):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.linear_layer = nn.Linear(in_features, hidden_dim).double().cuda()\n",
    "        self.relu = nn.ReLU().double().cuda()\n",
    "        self.clustering_layer = ClusteringLayer3(initial_centroids=initial_centroids)\n",
    "    def forward(self, X):\n",
    "        \n",
    "        return self.clustering_layer(self.relu(self.linear_layer(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "initial_centroids = torch.from_numpy(kmeans_centroids).cuda()\n",
    "clustering_layer = ClusteringLayer2(initial_centroids=initial_centroids).cuda()\n",
    "#test_model = TestModel(initial_centroids=initial_centroids).cuda()\n",
    "\n",
    "loss_fn = nn.KLDivLoss(reduction='sum')\n",
    "optimizer = torch.optim.AdamW(params=clustering_layer.parameters(), lr=0.001)\n",
    "\n",
    "X = torch.from_numpy(X).cuda()\n",
    "\n",
    "losses = []\n",
    "acc = []\n",
    "pbar = tqdm(range(2500))\n",
    "for i in pbar:\n",
    "    p, q, loss, accuracy = train_clusteringlayer(X=X, y=y,\n",
    "                                                 clustering_layer=clustering_layer,\n",
    "                                                 optimizer=optimizer,\n",
    "                                                 loss_fn=loss_fn)\n",
    "    \n",
    "    losses.append(loss)\n",
    "    acc.append(accuracy)\n",
    "    pbar.set_description(f'Accuracy {accuracy} | Loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(q.argmax(dim=1).cpu().detach().numpy(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "input_ids = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "outputs = model(input_ids, labels=input_ids)\n",
    "loss = outputs.loss\n",
    "prediction_logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-1.8296e-01, -7.4054e-02,  5.0267e-02,  ..., -1.1261e-01,\n",
       "           4.4493e-01,  4.0941e-01],\n",
       "         [ 7.0632e-04,  1.4825e-01,  3.4328e-01,  ..., -8.6040e-02,\n",
       "           6.9475e-01,  4.3353e-02],\n",
       "         [-5.0721e-01,  5.3086e-01,  3.7163e-01,  ..., -5.6287e-01,\n",
       "           1.3756e-01,  2.8475e-01],\n",
       "         ...,\n",
       "         [-4.2251e-01,  5.7314e-02,  2.4338e-01,  ..., -1.5223e-01,\n",
       "           2.4462e-01,  6.4155e-01],\n",
       "         [-4.9384e-01, -1.8895e-01,  1.2641e-01,  ...,  6.3241e-02,\n",
       "           3.6913e-01, -5.8252e-02],\n",
       "         [ 8.3269e-01,  2.4948e-01, -4.5440e-01,  ...,  1.1998e-01,\n",
       "          -3.9257e-01, -2.7785e-01]]], grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
